{"cells":[{"cell_type":"markdown","metadata":{"id":"1R7M24cel_lj"},"source":["# Connect to Google Drive and navigate to folder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19637,"status":"ok","timestamp":1710523653889,"user":{"displayName":"Mateus Bortoli Harano","userId":"06096344429142300984"},"user_tz":-60},"id":"2Mj7qn4ux80u","outputId":"3e87445a-8f13-4f73-f45e-0eb3e90fb3db"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710523653890,"user":{"displayName":"Mateus Bortoli Harano","userId":"06096344429142300984"},"user_tz":-60},"id":"FuUrf5_bSYPe","outputId":"70ef0500-f5bd-46da-db73-472d87de6c60"},"outputs":[],"source":["%cd /content/drive/MyDrive/deeplify"]},{"cell_type":"markdown","metadata":{"id":"sUKP8Bm5mJkH"},"source":["# Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16539,"status":"ok","timestamp":1710523670427,"user":{"displayName":"Mateus Bortoli Harano","userId":"06096344429142300984"},"user_tz":-60},"id":"lqgYD66u2CE8","outputId":"ed331e72-714e-431a-95dd-0ac8675faa8a"},"outputs":[],"source":["!pip install wandb\n","!wandb login ac108e79c3c8e424b4cb713b6c9c32aa83ce33cc"]},{"cell_type":"markdown","metadata":{"id":"zKT-TOyvmPPN"},"source":["# Load data to VM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27584,"status":"ok","timestamp":1710523698006,"user":{"displayName":"Mateus Bortoli Harano","userId":"06096344429142300984"},"user_tz":-60},"id":"hMppz8imVyDU","outputId":"2644f1b0-6296-4280-9b37-5187531b9bdf"},"outputs":[],"source":["!unzip chest_xray.zip -d /content/dataset"]},{"cell_type":"markdown","metadata":{"id":"6Raq5-SJmW8i"},"source":["# Train models"]},{"cell_type":"markdown","metadata":{},"source":["## ResNet18 Adam\n","\n","* weights name: Resnet-Adam(0.001)-batch(16).pth\n","* model: ResNet18\n","* optimizer: Adam\n","* learning rate: 0.001\n","* batch size: 16\n","* weighted loss not yet applied"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from train import train_model\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import Conv2d, Linear\n","import torch\n","from torchvision.models import resnet18, ResNet18_Weights\n","from utils import mount_dataset\n","\n","# Setup model\n","model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","n_classes = 2\n","model.conv1 = Conv2d(1, 64, kernel_size=(7,7), stride=(2, 2), padding=(3, 3), bias=False)\n","model.fc = Linear(in_features=512, out_features=n_classes, bias=True)\n","device = torch.device('cuda')\n","model.to(device, dtype=torch.float)\n","\n","# Setup data\n","classes, dataloaders = mount_dataset('/content/dataset/chest_xray', batch_size=16, num_workers=2)\n","\n","# Other training elements\n","optimizer = Adam(model.parameters(), lr=0.001, foreach=True)\n","criterion = CrossEntropyLoss()\n","\n","model = train_model(\n","    model=model,\n","    dataloaders=dataloaders,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    classes=classes,\n","    device=device,\n","    num_epochs=30\n","    )\n","\n","torch.save(model.state_dict(), 'Resnet-Adam(0.001)-batch(16).pth')"]},{"cell_type":"markdown","metadata":{},"source":["## ResNet18 SGD\n","\n","* weights name: ResNet18-SGD(0.001)-batch(32).pth\n","* model: ResNet18\n","* optimizer: SGD\n","* learning rate: 0.001\n","* batch size: 32\n","* weighted loss not yet applied"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"DSS8Pa0UhqUE","outputId":"c2d3cd2d-c633-4c33-bf13-c8034e6bf18a"},"outputs":[],"source":["from train import train_model\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import Conv2d\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch\n","from torchvision.models import resnet18, ResNet18_Weights\n","from utils import mount_dataset\n","from eval import evaluate\n","\n","# Setup model\n","model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","model.conv1 = Conv2d(1, 64, kernel_size=(7,7), stride=(2, 2), padding=(3, 3), bias=False)\n","n_classes = 2\n","num_ftrs = model.fc.in_features\n","model.fc = torch.nn.Linear(num_ftrs, n_classes, bias=True)\n","\n","device = torch.device('cuda')\n","model.to(device, dtype=torch.float)\n","\n","# Setup data\n","classes, dataloaders = mount_dataset('/content/dataset/chest_xray', batch_size=32, num_workers=2)\n","\n","# Other training elements\n","criterion = CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","model = train_model(\n","    model=model,\n","    dataloaders=dataloaders,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    classes=classes,\n","    device=device,\n","    num_epochs=20\n","    )\n","\n","torch.save(model.state_dict(), 'ResNet18-SGD(0.001)-batch(32).pth')\n","\n","evaluate(model, dataloaders['test'], device, classes)"]},{"cell_type":"markdown","metadata":{},"source":["## ResNet18 Weighted Loss\n","\n","* weights name: ResNet18-SGD(0.001)-batch(32)-weightedloss.pth\n","* model: ResNet18\n","* optimizer: SGD\n","* learning rate: 0.001\n","* batch size: 32\n","* weighted loss applied"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import Conv2d, Linear\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch\n","import numpy as np\n","import wandb\n","from torchvision.models import resnet18, ResNet18_Weights\n","from train import train_model\n","from utils import mount_dataset\n","from eval import evaluate\n","\n","# Setup model\n","model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","model.conv1 = Conv2d(1, 64, kernel_size=(7,7), stride=(2, 2), padding=(3, 3), bias=False)\n","n_classes = 2\n","num_ftrs = model.fc.in_features\n","model.fc = torch.nn.Linear(num_ftrs, n_classes, bias=False)\n","\n","device = torch.device('cuda')\n","model.to(device, dtype=torch.float)\n","\n","# Setup data\n","classes, dataloaders = mount_dataset('/content/dataset/chest_xray', batch_size=32, num_workers=2)\n","\n","# Other training elements\n","classes = ['NORMAL', 'PNEUMONIA']\n","data_distribution = np.array([{'NORMAL': 1341, 'PNEUMONIA': 3875}[cls] for cls in classes], dtype=float)\n","weights = np.linalg.norm(data_distribution)/data_distribution\n","weights /= weights.sum()\n","criterion = CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","model = train_model(\n","    model=model,\n","    dataloaders=dataloaders,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    classes=classes,\n","    device=device,\n","    num_epochs=20\n","    )\n","\n","torch.save(model.state_dict(), 'ResNet18-SGD(0.001)-batch(32)-weightedloss.pth')\n","\n","evaluate(model, dataloaders['test'], device, classes)"]},{"cell_type":"markdown","metadata":{},"source":["## Vision Transformer\n","\n","* weights name: ViTb16-SGD(0.001)-batch(4)-weightedloss.pth\n","* model: ViTb16\n","* optimizer: SGD\n","* learning rate: 0.001\n","* batch size: 4\n","* weighted loss applied"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.optim import Adam, SGD, lr_scheduler\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import Conv2d, Linear\n","import torch, math\n","import numpy as np\n","from torchvision.models import vit_b_16, ViT_B_16_Weights\n","from utils import mount_dataset\n","from train import train_model\n","from eval import evaluate\n","\n","image_size = 960/2\n","n_classes = 2\n","\n","# Setup model\n","model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n","model.image_size = image_size\n","n_feat = model.conv_proj.out_channels\n","n_embed = int(image_size/model.patch_size)\n","\n","# Adjust model to data\n","# Interpolate positional embedding\n","embedding = model.encoder.pos_embedding\n","tokens, image_sectors = embedding[:, :1], embedding[:, 1:]\n","n_sectors = int(math.sqrt(image_sectors.shape[1]))\n","image_sectors = image_sectors.reshape(1, n_sectors, n_sectors, -1).permute(0, 3, 1, 2)  # [1, 196, dim]->[1, 14, 14, dim]->[1, dim, 14, 14]\n","image_sectors = torch.nn.functional.interpolate(image_sectors, size=(n_embed, n_embed), mode='bicubic') # [1, dim, 14, 14] -> [1, dim, 24, 24]\n","image_sectors = image_sectors.permute(0, 2, 3, 1).reshape(1, n_embed * n_embed, -1)   # [1, dim, 24, 24] -> [1, 24*24, dim]\n","interpolated_embedding = torch.cat([tokens, image_sectors], dim=1)   # [1, 24*24+1, dim]\n","model.encoder.pos_embedding = torch.nn.Parameter(interpolated_embedding, True)\n","# Adjust channels and outputs\n","model.conv_proj = Conv2d(1, n_feat, kernel_size=(16, 16), stride=(16, 16))\n","model.heads.head = Linear(in_features=n_feat, out_features=n_classes, bias=False)\n","device = torch.device('cuda')\n","model = model.to(device)\n","\n","# Setup data\n","classes, dataloaders = mount_dataset('/content/dataset/chest_xray', batch_size=16, num_workers=2, image_size=(image_size,image_size), image_resize_scale=(490, 670))\n","\n","# Other training elements\n","data_distribution = np.array([{'NORMAL': 1341, 'PNEUMONIA': 3875}[cls] for cls in classes], dtype=float)\n","weights = np.linalg.norm(data_distribution)/data_distribution\n","weights /= weights.sum()\n","weights = torch.from_numpy(weights)\n","weights = weights.to(device, dtype=torch.float)\n","criterion = CrossEntropyLoss(weight=weights)\n","optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","model = train_model(\n","    model=model,\n","    dataloaders=dataloaders,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    classes=classes,\n","    device=device,\n","    num_epochs=20\n","    )\n","\n","torch.save(model.state_dict(), 'ViTb16-SGD(0.001)-batch(4)-weightedloss.pth')\n","\n","evaluate(model, dataloaders['test'], device, classes)"]},{"cell_type":"markdown","metadata":{},"source":["## EfficientNet v2\n","\n","* weights name: EfficientNetv2-SGD(0.001)-batch(4)-weightedloss.pth\n","* model: EfficientNet_v2_s\n","* optimizer: SGD\n","* learning rate: 0.001\n","* batch size: 4\n","* weighted loss applied"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from torch.nn import Conv2d\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch\n","import numpy as np\n","from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n","from train import train_model\n","from utils import mount_dataset\n","from eval import evaluate\n","\n","\n","# Setup model\n","model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n","model.features[0][0] = Conv2d(1, 24, kernel_size=(3,3), stride=(2, 2), padding=(1,1), bias=False)\n","n_classes = 2\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = torch.nn.Linear(num_ftrs, n_classes, bias=False)\n","\n","device = torch.device('cuda')\n","model.to(device, dtype=torch.float)\n","\n","# Setup data\n","image_size = 960/2\n","classes, dataloaders = mount_dataset('/content/dataset/chest_xray', batch_size=4, num_workers=2, image_size=(image_size,image_size), image_resize_scale=(490, 670))\n","\n","# Other training elements\n","data_distribution = np.array([{'NORMAL': 1341, 'PNEUMONIA': 3875}[cls] for cls in classes], dtype=float)\n","weights = np.linalg.norm(data_distribution)/data_distribution\n","weights /= weights.sum()\n","weights = torch.from_numpy(weights)\n","weights = weights.to(device, dtype=torch.float)\n","criterion = CrossEntropyLoss(weight=weights)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","model = train_model(\n","    model=model,\n","    dataloaders=dataloaders,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    classes=classes,\n","    device=device,\n","    num_epochs=20\n","    )\n","\n","torch.save(model.state_dict(), 'EfficientNetv2-SGD(0.001)-batch(4)-weightedloss.pth')\n","\n","evaluate(model, dataloaders['test'], device, classes)"]},{"cell_type":"markdown","metadata":{},"source":["## Swin v2\n","\n","* weights name: Swinv2-SGD(0.001)-batch(8).pth\n","* model: Swin_v2_s\n","* optimizer: SGD\n","* learning rate: 0.001\n","* batch size: 8\n","* weighted loss applied"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.nn import Conv2d, Linear\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch\n","import numpy as np\n","from torchvision.models import swin_v2_s, Swin_V2_S_Weights\n","from train import train_model\n","from utils import mount_dataset\n","from eval import evaluate\n","\n","# Setup model\n","model = swin_v2_s(weights=Swin_V2_S_Weights.IMAGENET1K_V1)\n","model.features[0][0] = Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n","model.head = Linear(in_features=model.head.in_features, out_features=2, bias=True)\n","\n","device = torch.device('cuda')\n","model.to(device, dtype=torch.float)\n","\n","# Setup data\n","image_size = 960/2\n","classes, dataloaders = mount_dataset('/content/dataset/chest_xray', batch_size=8, num_workers=2, image_size=(image_size,image_size), image_resize_scale=(490, 670))\n","\n","# Other training elements\n","data_distribution = np.array([{'NORMAL': 1341, 'PNEUMONIA': 3875}[cls] for cls in classes], dtype=float)\n","weights = np.linalg.norm(data_distribution)/data_distribution\n","weights /= weights.sum()\n","weights = torch.from_numpy(weights)\n","weights = weights.to(device, dtype=torch.float)\n","criterion = CrossEntropyLoss(weight=weights)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","model = train_model(\n","    model=model,\n","    dataloaders=dataloaders,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    classes=classes,\n","    device=device,\n","    num_epochs=20\n","    )\n","\n","torch.save(model.state_dict(), 'Swinv2-SGD(0.001)-batch(8).pth')\n","\n","evaluate(model, dataloaders['test'], device, classes)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPTRlUaC9CwRv1AszJG+YhR","gpuType":"T4","mount_file_id":"1EawzfrUVBhLRpgSke_tK8WoJFtUOBMJS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
